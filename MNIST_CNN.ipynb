{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statemements\n",
    "\n",
    "import random\n",
    "import pickle # used to save and restore python objects\n",
    "import gzip\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "EPOCHS = 5\n",
    "RESIZE_WIDTH = 28\n",
    "RESIZE_HEIGHT = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "\n",
    "f = gzip.open('./data/mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f, encoding = 'latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: to_categorical -- One-Hot Vector Encoding \n",
    "\n",
    "# Converts class labels (integers from 0 to nb_classes) to one-hot vector\n",
    "# Example: 5 => [0 0 0 0 0 1 0 0 0 0]\n",
    "\n",
    "# Arguments:\n",
    "    # y: array, class labels to convert\n",
    "    # nb_classes: integer, total number of classes \n",
    "\n",
    "def to_categorical(y, nb_classes):\n",
    "    y = np.asarray(y, dtype='int32')\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(y)+1\n",
    "    answer = np.zeros((len(y), nb_classes))\n",
    "    answer[np.arange(len(y)),y] = 1.\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "Train set has dimensionality (50000, 28, 28, 1)\n",
      "Test set has dimensionality (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split, One-Hot Encoding for our data\n",
    "\n",
    "train_x = train_set[0]\n",
    "train_y = to_categorical(train_set[1], 10)\n",
    "test_x = test_set[0]\n",
    "test_y = to_categorical(test_set[1],10)\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train_x = train_x.reshape(train_x.shape[0], 1, RESIZE_WIDTH, RESIZE_HEIGHT)\n",
    "    test_x = test_x.reshape(test_x.shape[0], 1, RESIZE_WIDTH, RESIZE_HEIGHT)\n",
    "    input_shape = (1, RESIZE_WIDTH, RESIZE_HEIGHT)\n",
    "else:\n",
    "    train_x = train_x.reshape(train_x.shape[0], RESIZE_WIDTH, RESIZE_HEIGHT, 1)\n",
    "    test_x = test_x.reshape(test_x.shape[0], RESIZE_WIDTH, RESIZE_HEIGHT, 1)\n",
    "    input_shape = (RESIZE_WIDTH, RESIZE_HEIGHT, 1)\n",
    "\n",
    "# Result of One-Hot Encoding Class Labels \n",
    "print(test_y[:5])\n",
    "\n",
    "print('Train set has dimensionality %s' % str(train_x.shape))\n",
    "print('Test set has dimensionality %s' % str(test_x.shape))\n",
    "\n",
    "# Apply some normalization here.\n",
    "train_x = train_x.astype('float32')\n",
    "test_x = test_x.astype('float32')\n",
    "train_x /= 255\n",
    "test_x /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Import necessary layers.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.constraints import max_norm\n",
    "######################################\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "######################################\n",
    "# Define the network\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='glorot_normal', input_shape=input_shape))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(512, kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "######################################\n",
    "\n",
    "\n",
    "######################################\n",
    "# Define your loss and your objective\n",
    "optimizer = RMSprop(lr=1e-4)\n",
    "loss = 'binary_crossentropy'\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "40000/40000 [==============================] - 270s 7ms/step - loss: 0.2628 - acc: 0.9117 - val_loss: 0.1156 - val_acc: 0.9578\n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 273s 7ms/step - loss: 0.0947 - acc: 0.9652 - val_loss: 0.0644 - val_acc: 0.9769\n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 274s 7ms/step - loss: 0.0644 - acc: 0.9773 - val_loss: 0.0490 - val_acc: 0.9825\n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 263s 7ms/step - loss: 0.0502 - acc: 0.9827 - val_loss: 0.0404 - val_acc: 0.9859\n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 273s 7ms/step - loss: 0.0413 - acc: 0.9859 - val_loss: 0.0362 - val_acc: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x191827cc908>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################\n",
    "# Define the batch size\n",
    "batch_size = 64\n",
    "######################################\n",
    "model.fit(train_x, train_y, batch_size=batch_size, epochs=EPOCHS, validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 20s 2ms/step\n",
      "\n",
      "Got 98.94% accuracy\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Performance\n",
    "loss, acc = model.evaluate(test_x, test_y, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('')\n",
    "print('Got %.2f%% accuracy' % (acc * 100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
